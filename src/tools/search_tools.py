"""
MCP æœç´¢å·¥å…·
===========

æ­¤æ¨¡å—åŒ…å«ä¸çŸ¥è¯†åº“æœç´¢å’ŒæŸ¥è¯¢ç›¸å…³çš„å·¥å…·ã€‚
ä» rag_server.py è¿ç§»è€Œæ¥ï¼Œç”¨äºæ¨¡å—åŒ–æ¶æ„ã€‚

æ³¨æ„ï¼šè¿™äº›å‡½æ•°è¢«è®¾è®¡ä¸ºåœ¨ä¸»æœåŠ¡å™¨ä¸­ä½¿ç”¨ @mcp.tool() è£…é¥°å™¨ã€‚
"""

from rag_core_openai import (
    get_qa_chain,
    create_metadata_filter
)
from utils.logger import log

# å¯¼å…¥ç»“æ„åŒ–æ¨¡å‹
try:
    from models import MetadataModel
except ImportError as e:
    log(f"è­¦å‘Šï¼šæ— æ³•å¯¼å…¥ç»“æ„åŒ–æ¨¡å‹ï¼š{e}")
    MetadataModel = None

# å¿…é¡»åœ¨æœåŠ¡å™¨ä¸­å¯ç”¨çš„å…¨å±€å˜é‡
rag_state = {}
initialize_rag_func = None

def set_rag_state(state):
    """è®¾ç½®å…¨å±€ RAG çŠ¶æ€ã€‚"""
    global rag_state
    rag_state = state

def set_initialize_rag_func(func):
    """è®¾ç½® RAG åˆå§‹åŒ–å‡½æ•°ã€‚"""
    global initialize_rag_func
    initialize_rag_func = func

def initialize_rag():
    """åˆå§‹åŒ– RAG ç³»ç»Ÿã€‚"""
    if initialize_rag_func:
        initialize_rag_func()
    elif "initialized" in rag_state:
        return
    # æ­¤å‡½æ•°å¿…é¡»åœ¨ä¸»æœåŠ¡å™¨ä¸­å®ç°
    pass

def process_document_metadata(metadata: dict) -> dict:
    """
    ä½¿ç”¨ MetadataModelï¼ˆå¦‚æœå¯ç”¨ï¼‰å¤„ç†æ–‡æ¡£å…ƒæ•°æ®ã€‚
    
    å‚æ•°ï¼š
        metadata: æ–‡æ¡£å…ƒæ•°æ®å­—å…¸
        
    è¿”å›ï¼š
        åŒ…å«å·²å¤„ç†æ–‡æ¡£ä¿¡æ¯çš„å­—å…¸
    """
    if not metadata:
        return {"source": "æœªçŸ¥æ¥æº"}
    
    # å¦‚æœ MetadataModel å¯ç”¨ï¼Œå°è¯•åˆ›å»ºç»“æ„åŒ–æ¨¡å‹
    if MetadataModel is not None:
        try:
            metadata_model = MetadataModel.from_dict(metadata)
            return {
                "source": metadata_model.source,
                "file_path": metadata_model.file_path,
                "file_type": metadata_model.file_type,
                "processing_method": metadata_model.processing_method,
                "structural_info": metadata_model.structural_info,
                "titles_count": metadata_model.titles_count,
                "tables_count": metadata_model.tables_count,
                "lists_count": metadata_model.lists_count,
                "total_elements": metadata_model.total_elements,
                "is_rich_content": metadata_model.is_rich_content(),
                "chunking_method": metadata_model.chunking_method,
                "avg_chunk_size": metadata_model.avg_chunk_size
            }
        except Exception as e:
            log(f"MCPæœåŠ¡å™¨è­¦å‘Šï¼šä½¿ç”¨ MetadataModel å¤„ç†å…ƒæ•°æ®æ—¶å‡ºé”™ï¼š{e}")
    
    # å›é€€åˆ°ç›´æ¥å­—å…¸å¤„ç†
    return {
        "source": metadata.get("source", "æœªçŸ¥æ¥æº"),
        "file_path": metadata.get("file_path"),
        "file_type": metadata.get("file_type"),
        "processing_method": metadata.get("processing_method"),
        "structural_info": metadata.get("structural_info", {}),
        "titles_count": metadata.get("structural_titles_count", 0),
        "tables_count": metadata.get("structural_tables_count", 0),
        "lists_count": metadata.get("structural_lists_count", 0),
        "total_elements": metadata.get("structural_total_elements", 0),
        "is_rich_content": False,  # æ²¡æœ‰æ¨¡å‹æ— æ³•ç¡®å®š
        "chunking_method": metadata.get("chunking_method", "æœªçŸ¥"),
        "avg_chunk_size": metadata.get("avg_chunk_size", 0)
    }


def extract_brief_answer(full_text: str) -> str:
    """
    ä»å¢å¼ºå›ç­”æ–‡æœ¬ä¸­æå–ç®€æ´å›ç­”ï¼ˆå»æ‰å‰ç¼€ã€æ¥æºå’Œå»ºè®®éƒ¨åˆ†ï¼‰ã€‚
    è¿”å›å»æ‰æ‚é¡¹åçš„çº¯æ–‡æœ¬ï¼ˆå¦‚æœæ— æ³•æå–åˆ™è¿”å›åŸæ–‡çš„ç®€çŸ­å½¢å¼æˆ–ç©ºå­—ç¬¦ä¸²ï¼‰ã€‚
    """
    if not full_text:
        return ""

    text = full_text.strip()

    # å¸¸è§å‰ç¼€
    prefixes = ["ğŸ¤– å›ç­”ï¼š", "ğŸ” å›ç­”ï¼ˆå·²åº”ç”¨è¿‡æ»¤å™¨ï¼‰ï¼š", "ğŸ” å›ç­”ï¼š", "å›ç­”ï¼š"]
    for p in prefixes:
        if text.startswith(p):
            text = text[len(p):].lstrip('\n ').lstrip()
            break

    # æˆªæ–­åˆ°ç¬¬ä¸€ä¸ªæ¥æºæˆ–å»ºè®®æ ‡è®°
    for marker in ["ğŸ“š ä½¿ç”¨çš„ä¿¡æ¯æ¥æºï¼š", "ğŸ“‹ åº”ç”¨çš„è¿‡æ»¤å™¨ï¼š", "ğŸ’¡ å»ºè®®ï¼š", "âš ï¸ æ³¨æ„ï¼š"]:
        idx = text.find(marker)
        if idx != -1:
            text = text[:idx].rstrip()
            break

    return text.strip()

def ask_rag(query: str) -> str:
    """
    å‘ RAG çŸ¥è¯†åº“æé—®å¹¶åŸºäºå­˜å‚¨çš„ä¿¡æ¯è¿”å›ç­”æ¡ˆã€‚
    å½“æ‚¨æƒ³ä»ä¹‹å‰å­¦ä¹ çš„çŸ¥è¯†åº“ä¸­è·å–ä¿¡æ¯æ—¶ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚
    
    ä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š
    - è¯¢é—®ç‰¹å®šä¸»é¢˜æˆ–æ¦‚å¿µ
    - è¯·æ±‚è§£é‡Šæˆ–å®šä¹‰
    - ä»å¤„ç†è¿‡çš„æ–‡æ¡£ä¸­å¯»æ±‚ä¿¡æ¯
    - åŸºäºå­¦ä¹ çš„æ–‡æœ¬æˆ–æ–‡æ¡£è·å–ç­”æ¡ˆ
    
    ç³»ç»Ÿå°†æœç´¢æ‰€æœ‰å­˜å‚¨çš„ä¿¡æ¯å¹¶æä¾›æœ€ç›¸å…³çš„ç­”æ¡ˆã€‚

    å‚æ•°ï¼š
        query: å‘çŸ¥è¯†åº“æå‡ºçš„é—®é¢˜æˆ–æŸ¥è¯¢ã€‚
    """
    log(f"MCPæœåŠ¡å™¨ï¼šæ­£åœ¨å¤„ç†é—®é¢˜ï¼š{query}")
    initialize_rag()
    
    try:
        # ä½¿ç”¨æ ‡å‡† QA é“¾ï¼ˆæ— è¿‡æ»¤å™¨ï¼‰
        qa_chain = get_qa_chain(rag_state["vector_store"])
        response = qa_chain.invoke({"query": query})
        
        answer = response.get("result", "")
        source_documents = response.get("source_documents", [])

        # ä¼˜å…ˆè¿”å›ç®€æ´çš„å›ç­”æ–‡æœ¬ï¼ˆå»æ‰æ¥æºä¸å»ºè®®ï¼‰
        concise = extract_brief_answer(response.get("result", ""))
        if concise:
            log(f"MCPæœåŠ¡å™¨ï¼šæˆåŠŸç”Ÿæˆç®€æ´å›ç­”ï¼Œä½¿ç”¨äº† {len(source_documents)} ä¸ªæ¥æº")
            return concise
        
        # éªŒè¯æ˜¯å¦çœŸçš„æœ‰ç›¸å…³ä¿¡æ¯
        if not source_documents:
            # æ²¡æœ‰æ¥æº - LLM å¯èƒ½åœ¨äº§ç”Ÿå¹»è§‰
            enhanced_answer = f"ğŸ¤– å›ç­”ï¼š\n\nâŒ åœ¨çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚\n\n"
            enhanced_answer += "ğŸ’¡ å»ºè®®ï¼š\n"
            enhanced_answer += "â€¢ éªŒè¯æ‚¨æ˜¯å¦å·²åŠ è½½ä¸é—®é¢˜ç›¸å…³çš„æ–‡æ¡£\n"
            enhanced_answer += "â€¢ å°è¯•ç”¨æ›´å…·ä½“çš„æœ¯è¯­é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜\n"
            enhanced_answer += "â€¢ ä½¿ç”¨ `get_knowledge_base_stats()` æŸ¥çœ‹å¯ç”¨ä¿¡æ¯\n"
            enhanced_answer += "â€¢ è€ƒè™‘åŠ è½½æ›´å¤šå…³äºæ‚¨æ„Ÿå…´è¶£ä¸»é¢˜çš„æ–‡æ¡£\n\n"
            enhanced_answer += "âš ï¸ æ³¨æ„ï¼š ç³»ç»Ÿåªèƒ½åŸºäºä¹‹å‰åŠ è½½åˆ°çŸ¥è¯†åº“ä¸­çš„ä¿¡æ¯è¿›è¡Œå›ç­”ã€‚"
            
            log(f"MCPæœåŠ¡å™¨ï¼šæœªæ‰¾åˆ°ç›¸å…³æ¥æºå›ç­”é—®é¢˜")
            return enhanced_answer
        
        # éªŒè¯å›ç­”æ˜¯å¦å¯èƒ½æ˜¯å¹»è§‰
        # å¦‚æœæ²¡æœ‰æ¥æºä½†æœ‰å›ç­”ï¼Œå¯èƒ½æ˜¯å¹»è§‰
        if len(source_documents) == 0 and answer.strip():
            enhanced_answer = f"ğŸ¤– å›ç­”ï¼š\n\nâŒ åœ¨çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç‰¹å®šä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚\n\n"
            enhanced_answer += "ğŸ’¡ å»ºè®®ï¼š\n"
            enhanced_answer += "â€¢ éªŒè¯æ‚¨æ˜¯å¦å·²åŠ è½½ä¸é—®é¢˜ç›¸å…³çš„æ–‡æ¡£\n"
            enhanced_answer += "â€¢ å°è¯•ç”¨æ›´å…·ä½“çš„æœ¯è¯­é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜\n"
            enhanced_answer += "â€¢ ä½¿ç”¨ `get_knowledge_base_stats()` æŸ¥çœ‹å¯ç”¨ä¿¡æ¯\n\n"
            enhanced_answer += "âš ï¸ æ³¨æ„ï¼š ç³»ç»Ÿåªèƒ½åŸºäºä¹‹å‰åŠ è½½åˆ°çŸ¥è¯†åº“ä¸­çš„ä¿¡æ¯è¿›è¡Œå›ç­”ã€‚"
            
            log(f"MCPæœåŠ¡å™¨ï¼šæ£€æµ‹åˆ°å¯èƒ½çš„å¹»è§‰å›ç­”ï¼ˆæ— æ¥æºï¼‰")
            return enhanced_answer
        
        # å¦‚æœæœ‰æ¥æºï¼Œæ„å»ºæ­£å¸¸å›ç­”
        enhanced_answer = f"ğŸ¤– å›ç­”ï¼š\n\n{answer}\n"
        
        # ä½¿ç”¨ç»“æ„åŒ–æ¨¡å‹æ·»åŠ æ›´è¯¦ç»†çš„æ¥æºä¿¡æ¯
        if source_documents:
            enhanced_answer += "ğŸ“š ä½¿ç”¨çš„ä¿¡æ¯æ¥æºï¼š\n\n"
            for i, doc in enumerate(source_documents, 1):
                raw_metadata = doc.metadata if hasattr(doc, 'metadata') else {}
                
                # ä½¿ç”¨ç»“æ„åŒ–æ¨¡å‹å¤„ç†å…ƒæ•°æ®
                doc_info = process_document_metadata(raw_metadata)
                
                # --- æ”¹è¿›æ¥æºä¿¡æ¯ ---
                source_info = f"   {i}. {doc_info['source']}"
                
                # å¦‚æœæ˜¯æ–‡æ¡£ï¼Œæ·»åŠ å®Œæ•´è·¯å¾„
                if doc_info['file_path']:
                    source_info += f"\n      - è·¯å¾„ï¼š `{doc_info['file_path']}`"
                
                # å¦‚æœå¯ç”¨ï¼Œæ·»åŠ æ–‡ä»¶ç±»å‹
                if doc_info['file_type']:
                    source_info += f"\n      - ç±»å‹ï¼š {(doc_info.get('file_type') or 'unknown').upper()}"
                
                # å¦‚æœå¯ç”¨ï¼Œæ·»åŠ å¤„ç†æ–¹æ³•
                if doc_info['processing_method']:
                    method_display = doc_info['processing_method'].replace('_', ' ').title()
                    source_info += f"\n      - å¤„ç†ï¼š {method_display}"
                
                # ä½¿ç”¨æ¨¡å‹æ•°æ®æ·»åŠ ç»“æ„ä¿¡æ¯
                if doc_info['total_elements'] > 0:
                    source_info += f"\n      - ç»“æ„ï¼š {doc_info['total_elements']} ä¸ªå…ƒç´ "
                    
                    structural_details = []
                    if doc_info['titles_count'] > 0:
                        structural_details.append(f"{doc_info['titles_count']} ä¸ªæ ‡é¢˜")
                    if doc_info['tables_count'] > 0:
                        structural_details.append(f"{doc_info['tables_count']} ä¸ªè¡¨æ ¼")
                    if doc_info['lists_count'] > 0:
                        structural_details.append(f"{doc_info['lists_count']} ä¸ªåˆ—è¡¨")
                    
                    if structural_details:
                        source_info += f" ({', '.join(structural_details)})"
                
                # å¦‚æœå¯ç”¨ï¼Œæ·»åŠ åˆ†å—ä¿¡æ¯
                if doc_info['chunking_method'] and doc_info['chunking_method'] != "æœªçŸ¥":
                    chunking_display = doc_info['chunking_method'].replace('_', ' ').title()
                    source_info += f"\n      - åˆ†å—ï¼š {chunking_display}"
                
                # å¦‚æœå¯ç”¨ï¼Œæ·»åŠ ä¸°å¯Œå†…å®¹æŒ‡ç¤ºå™¨
                if doc_info.get('is_rich_content', False):
                    source_info += f"\n      - è´¨é‡ï¼š ç»“æ„ä¸°å¯Œçš„å†…å®¹"
                
                enhanced_answer += source_info + "\n\n"
        
        # æ·»åŠ å›ç­”è´¨é‡ä¿¡æ¯
        num_sources = len(source_documents)
        if num_sources >= 3:
            enhanced_answer += "\nâœ… é«˜å¯ä¿¡åº¦ï¼š åŸºäºå¤šä¸ªæ¥æºçš„å›ç­”"
        elif num_sources == 2:
            enhanced_answer += "\nâš ï¸ ä¸­ç­‰å¯ä¿¡åº¦ï¼š åŸºäº 2 ä¸ªæ¥æºçš„å›ç­”"
        else:
            enhanced_answer += "\nâš ï¸ æœ‰é™å¯ä¿¡åº¦ï¼š åŸºäº 1 ä¸ªæ¥æºçš„å›ç­”"
        
        # ä½¿ç”¨ç»“æ„åŒ–æ¨¡å‹æ·»åŠ å¤„ç†ä¿¡æ¯
        enhanced_docs = []
        rich_content_docs = []
        
        for doc in source_documents:
            if hasattr(doc, 'metadata') and doc.metadata:
                doc_info = process_document_metadata(doc.metadata)
                if doc_info['processing_method'] == "unstructured_enhanced":
                    enhanced_docs.append(doc)
                if doc_info.get('is_rich_content', False):
                    rich_content_docs.append(doc)
        
        if enhanced_docs:
            enhanced_answer += f"\nğŸ§  æ™ºèƒ½å¤„ç†ï¼š {len(enhanced_docs)} ä¸ªæ¥æºä½¿ç”¨ Unstructured å¤„ç†ï¼ˆä¿ç•™ç»“æ„ï¼‰"
        
        if rich_content_docs:
            enhanced_answer += f"\nğŸ“Š ç»“æ„åŒ–å†…å®¹ï¼š {len(rich_content_docs)} ä¸ªæ¥æºå…·æœ‰ä¸°å¯Œç»“æ„ï¼ˆæ ‡é¢˜ã€è¡¨æ ¼ã€åˆ—è¡¨ï¼‰"
        
        log(f"MCPæœåŠ¡å™¨ï¼šæˆåŠŸç”Ÿæˆå›ç­”ï¼Œä½¿ç”¨äº† {len(source_documents)} ä¸ªæ¥æº")
        return enhanced_answer
        
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨ï¼šå¤„ç†é—®é¢˜æ—¶å‡ºé”™ï¼š{e}")
        return f"âŒ å¤„ç†é—®é¢˜æ—¶å‡ºé”™ï¼š {e}\n\nğŸ’¡ å»ºè®®ï¼š\n- éªŒè¯ RAG ç³»ç»Ÿæ˜¯å¦æ­£ç¡®åˆå§‹åŒ–\n- å°è¯•é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜\n- å¦‚æœé—®é¢˜æŒç»­å­˜åœ¨ï¼Œè¯·é‡å¯æœåŠ¡å™¨" 